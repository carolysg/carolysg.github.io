<!DOCTYPE HTML>
<!--
	Strata by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title>HG Weather API project</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="icon" type="image/x-icon" href="/images/icons/favicon.ico">
	<link rel="stylesheet" href="assets/css/main.css" />

	<link rel="stylesheet" href="assets/css/prism.css" />
</head>

<body class="is-preload">
	<script>
		document.body.classList.add('fade');

		document.addEventListener("DOMContentLoaded", () => {
			window.setTimeout(function () {
				document.body.classList.remove('fade');
			}, 230);
		});
	</script>
	<!-- Header -->
	<header id="header">
		<div class="bg-header"></div>
		<div class="inner">
			<a href="#" class="image avatar"><img src="images/img01.png" alt="" /></a>
			<ul class="sidebar">
				<li><a href="index.html" class="sidebar-link">Home</a></li>
				<li><a href="sobre.html" class="sidebar-link">About me</a></li>
				<li><a href="portfolio.html" class="sidebar-link">Projects</a></li>
				<li><a href="certificados.html" class="sidebar-link">Certificates</a></li>
			</ul>
		</div>
	</header>

	<!-- Main -->
	<div id="main">
		<section id="projeto-header">
			<header class="major">
				<div class="col-12"></div>
				<h2>Data collection via HG Weather API using GCP</h2>
			</header>
			<ul class="actions fit small">
				<li>
					<a href="https://github.com/carolysg/api-weather/tree/main"
						class="button icon brands small fit fa-github solid" target="_blank">Github repository</a>
				</li>
				<li>
					<a href="https://hgbrasil.com/status/weather" class="button fit small" target="_blank">API website</a>
				</li>
			</ul>
		</section>
		<!-- Conteúdo vai aqui -->
		<section id="sobre">
			<!-- Sobre o projeto -->
			<h2>About the project</h2>
			<p> <span class="image right"><img src="images/thumbs_proj/apihg.jpg" alt=""></span>
				This project was carried out entirely on the Google Cloud Platform (GCP) and consists of the following stages:
			</p>
			<ul>
				<li>Collecting weather data for the city of São Carlos (current and forecast data) using the HG Weather API.</li>
				<li>Processing data from the bronze layer to the silver layer.</li>
				<li>Uploading the data to BigQuery.</li>
			</ul>
			<p> The project architecture is presented in the diagram below: </p>
			<span class="image fit no-overlay"><img src="images/projetos/projeto-api/05_01.png" alt=""></span>
			<p> It's worth noting that, since the project was carried out entirely for free (using GCP's 90-day trial period), the cloud functions only ran between 10/15/2023 and 01/14/2024. </p>


			<!-- Etapas iniciais -->
			<h2>Initial steps</h2>
			<p><span class="image right right-bigger no-overlay"><img src="images/projetos/projeto-api/05_02.png"
						alt="" /></span>The first step of the project was to understand how the HG Weather API works, which would be used to collect weather data for the city of São Carlos. From the documentation, it was found that each city has a WOEID code and that this type of query did not require an API key. It was only necessary to search for the desired city, and then the link for using the API was automatically generated.</p>
			<p>When making a request using the API link, a JSON is returned with two types of information:
			</p>
			<ul>
				<li>Current weather data: date, time, temperature, weather description, humidity, wind speed, among other information.</li>

				<li>Weather forecast data for the next 9 days: date, day of the week, maximum and minimum temperatures, chance of rain, and more.</li>
			</ul>

			<p>An example of a JSON file obtained can be viewed at this link: <a
				href="https://github.com/carolysg/api-weather/blob/main/sao-carlos-2023-10-15-15-30.json" target="_blank">arquivo.json</a>
			</p>
			<p>Thus, after understanding the data format, the extraction phase was started.</p>

			<!-- Extração e ingestão dos dados -->
			<h2>Data extraction and ingestion</h2>
			<p>
				The cloud function <code>get_sao_carlos_weather</code>, written in Python, was responsible for making the data request and saving the JSON file in the <code>dados-api</code> bucket, inside the bronze folder. This function was triggered once a day by a cloud scheduler. Each day, the function saved the JSON file with the corresponding date.
			</p>
			<pre><code class="language-python">import functions_framework
import pandas as pd
import requests
import json
from datetime import datetime
from google.cloud import storage
					
# Triggered from a message on a Cloud Pub/Sub topic.
@functions_framework.cloud_event
def get_sao_carlos_weather(cloud_event=None):
					
	today = datetime.now()
	today = today.strftime('%Y-%m-%d-%H-%M')
	bucket_name = 'dados-api'
	destination_blob_name = f'bronze/sao-carlos-{today}.json'
						
	sao_carlos = requests.get('https://api.hgbrasil.com/weather?woeid=449704')
	json_data = sao_carlos.text
						
	storage_client = storage.Client()
	bucket = storage_client.get_bucket(bucket_name)
	blob = bucket.blob(destination_blob_name)
	blob.upload_from_string(json_data)</code></pre>

			<!-- Processamento dos dados -->
			</br>
			<h2>Data processing</h2>
			<p>
				The project architecture is entirely event-driven. As soon as a file was uploaded to the bronze folder, a Pub/Sub topic was triggered, activating the next cloud function.
			</p>
			<p>
				All the necessary transformations were carried out by the cloud function <code>process_sao_carlos_weather</code>. This function first verified the object saved in the bucket. If the object had the correct prefix, the data processing was initiated.
			</p>
			<pre><code class="language-python">
import functions_framework
import pandas as pd
import json
from datetime import timedelta
from google.cloud import storage
				
# Triggered from a message on a Cloud Pub/Sub topic.
@functions_framework.cloud_event
def process_sao_carlos_weather(cloud_event=None):
				
	bucket_name = cloud_event.data['message']['attributes']['bucketId']
	obj = cloud_event.data['message']['attributes']['objectId']
	uri = 'gs://' + bucket_name + '/' + obj
					
	# If for some reason function is triggered with another file abort
	if 'bronze/sao-carlos' not in obj:
	print('Not the expected file')
	return {'Message': 'Not the expected file'}

	storage_client = storage.Client()
	bucket = storage_client.get_bucket(bucket_name)
	blob = bucket.blob(obj)
	json_data = blob.download_as_text()
	data = json.loads(json_data)				
			</code></pre>
			</br>
			<p>
			Initially, the information contained in the JSON file was split into two dataframes: one for the current weather data and another for the forecast data. Only some of the information present in the JSON file was selected (those considered the most relevant).
		</p>
			<pre><code class="language-python">
storage_client = storage.Client()
bucket = storage_client.get_bucket(bucket_name)
blob = bucket.blob(obj)
json_data = blob.download_as_text()
data = json.loads(json_data)

results = data['results']
forecast = results['forecast']

df_real = pd.DataFrame({
	'date': [results['date']],
	'time': [results['time']],
	'temperature': [results['temp']],
	'description': [results['description']],
	'humidity': [results['humidity']],
	'cloudiness': [results['cloudiness']],
	'rain': [results['rain']],
	'wind_speedy': [results['wind_speedy']]
})

df_forecast = pd.DataFrame({
	'real_date': results['date'],
	'date': results['date'],
	'max_temperature': [item['max'] for item in forecast],
	'min_temperature': [item['min'] for item in forecast],
	'cloudiness': [item['cloudiness'] for item in forecast],
	'rain': [item['rain'] for item in forecast],
	'rain_probability': [item['rain_probability'] for item in forecast],
	'wind_speedy': [item['wind_speedy'] for item in forecast],
	'description': [item['description'] for item in forecast],
	'condition': [item['condition'] for item in forecast]
})
			</code></pre>
			</br>
			<p> The following transformations were performed: </p>
			<ul>
				<li>For the <code>wind_speedy</code> column, only the numeric values of the wind speed were extracted, removing the <code>km/h</code> characters.</li>
				<li>The data types of the columns were adjusted to facilitate further manipulation.</li>
				<li>The <code>date</code> column in the forecast dataframe was properly formatted.</li>
				<li>The <code>date</code> and <code>time</code> columns in the current weather dataframe were merged into a single <code>datetime</code> column and then dropped.</li>
			</ul>
			<pre><code class="language-python">
# == wind_speedy ==
df_real['wind_speedy'] = df_real['wind_speedy'].str.extract(r'(\d+\.\d+)')
df_forecast['wind_speedy'] = df_forecast['wind_speedy'].str.extract(r'(\d+\.\d+)')

# == types of columns ==
df_real['datetime'] = pd.to_datetime(df_real['date'] + ' ' + df_real['time'], dayfirst=True)
df_real['temperature'] = df_real['temperature'].astype(float)
df_real['humidity'] = df_real['humidity'].astype(float)
df_real['wind_speedy'] = df_real['wind_speedy'].astype(float)
df_forecast['real_date'] = pd.to_datetime(df_forecast['real_date'], format='%d/%m/%Y')
df_forecast['date'] = pd.to_datetime(df_forecast['date'], format='%d/%m/%Y')
df_forecast['max_temperature'] = df_forecast['max_temperature'].astype(float)
df_forecast['min_temperature'] = df_forecast['min_temperature'].astype(float)
df_forecast['rain_probability'] = df_forecast['rain_probability'].astype(float)
df_forecast['wind_speedy'] = df_forecast['wind_speedy'].astype(float)

# == date ==
first_date = df_forecast['date'].iloc[0]
for i in range(len(df_forecast)):
	df_forecast.loc[i, 'date'] = first_date + timedelta(days=i)

# == drop columns ==
df_real.drop(columns=['date', 'time'], inplace=True)
			</code></pre>
			</br>
			<p>
				After performing all these transformations, the dataframes were saved as separate CSV files—one for the current weather data and another for the forecast data. These files were saved in the silver folder of the same bucket, separated into the forecast and real folders.
			</p>
			<pre><code class="language-python">
# == save dataframes ==
name_file = obj.split('/')[-1]
name_file = name_file.replace('json', 'csv')
df_real.to_csv(f'gs://{bucket_name}/silver/real/real-{name_file}', index=False)
df_forecast.to_csv(f'gs://{bucket_name}/silver/forecast/forecast-{name_file}', index=False)
			</code></pre>
			</br>

			<!-- Upload dos dados no BigQuery -->
			<h2>Data upload to BigQuery</h2>
			<p>
				Similar to the data processing cloud function, the cloud functions for uploading data to BigQuery were triggered as soon as a file was uploaded to their respective folders. The functions <code>upload_forecast_sao_carlos_weather</code> and <code>upload_real_sao_carlos_weather</code> started in the same way as the previous function: by verifying the object saved in the bucket. If the object had the correct prefix, the data upload to BigQuery was initiated.
			</p>
			<p>
				Both tables in BigQuery, <code>tempo_real</code> and <code>tempo_forecast</code>, were created with a predefined schema. Additionally, they were set up with the <code>WRITE_APPEND</code> configuration, ensuring that data was always appended to the end of the table rather than replacing it. In this way, the tables would never be overwritten, but would always be appended with one or more rows per day.
			</p>
			<pre><code class="language-python">
import functions_framework from google.cloud 
import bigquery 
import pandas as pd 
import gcsfs 

# Triggered from a message on a Cloud Pub/Sub topic. 
@functions_framework.cloud_event 
def upload_real_sao_carlos_weather(cloud_event=None): 
	bucket_name = cloud_event.data['message']['attributes']['bucketId'] 
	obj = cloud_event.data['message']['attributes']['objectId'] 
	uri = 'gs://' + bucket_name + '/' + obj 

	# If for some reason function is triggered with another file abort 
	if 'silver/real/real-sao-carlos' not in obj: 
		print('Not the expected file') 
		return {'Message': 'Not the expected file'} 
		
	table_id = 'previsao-tempo-402114.dados_clima.tempo_real' 
	df = pd.read_csv(uri) 
	
	client = bigquery.Client() 
	job_config = bigquery.LoadJobConfig( 
		schema=[ 
			bigquery.SchemaField('temperature', bigquery.enums.SqlTypeNames.FLOAT), 
			bigquery.SchemaField('description', bigquery.enums.SqlTypeNames.STRING), 
			bigquery.SchemaField('humidity', bigquery.enums.SqlTypeNames.FLOAT), 
			bigquery.SchemaField('cloudiness', bigquery.enums.SqlTypeNames.FLOAT), 
			bigquery.SchemaField('rain', bigquery.enums.SqlTypeNames.FLOAT), 
			bigquery.SchemaField('wind_speedy', bigquery.enums.SqlTypeNames.FLOAT, description='km/h'), 
			bigquery.SchemaField('datetime', bigquery.enums.SqlTypeNames.DATETIME) 
		], 
		write_disposition='WRITE_APPEND', 
		autodetect=False, 
		source_format=bigquery.SourceFormat.CSV 
	) 
		
	job = client.load_table_from_dataframe(df, table_id, job_config=job_config) 
		
	return None
			</code></pre>
			</br>
			<pre><code class="language-python">
import functions_framework from google.cloud 
import bigquery 
import pandas as pd 
import gcsfs 

# Triggered from a message on a Cloud Pub/Sub topic. 
@functions_framework.cloud_event 
def upload_forecast_sao_carlos_weather(cloud_event=None): 
	bucket_name = cloud_event.data['message']['attributes']['bucketId'] 
	obj = cloud_event.data['message']['attributes']['objectId'] 
	uri = 'gs://' + bucket_name + '/' + obj 
	
	# If for some reason function is triggered with another file abort 
	if 'silver/forecast/forecast-sao-carlos' not in obj: 
		print('Not the expected file') 
		return {'Message': 'Not the expected file'} 
	
	table_id = 'previsao-tempo-402114.dados_clima.tempo_forecast' 
	df = pd.read_csv(uri) 

	client = bigquery.Client() 
	job_config = bigquery.LoadJobConfig( 
		schema=[ 
			bigquery.SchemaField('real_date', bigquery.enums.SqlTypeNames.DATE), 
			bigquery.SchemaField('date', bigquery.enums.SqlTypeNames.DATE), 
			bigquery.SchemaField('max_temperature', bigquery.enums.SqlTypeNames.FLOAT), 
			bigquery.SchemaField('min_temperature', bigquery.enums.SqlTypeNames.FLOAT), 
			bigquery.SchemaField('cloudiness', bigquery.enums.SqlTypeNames.FLOAT), 
			bigquery.SchemaField('rain', bigquery.enums.SqlTypeNames.FLOAT), 
			bigquery.SchemaField('rain_probability', bigquery.enums.SqlTypeNames.FLOAT), 
			bigquery.SchemaField('wind_speedy', bigquery.enums.SqlTypeNames.FLOAT, description='km/h'), 
			bigquery.SchemaField('description', bigquery.enums.SqlTypeNames.STRING), 
			bigquery.SchemaField('condition', bigquery.enums.SqlTypeNames.STRING) 
		], 
		write_disposition='WRITE_APPEND', 
		autodetect=False, 
		source_format=bigquery.SourceFormat.CSV 
	) 

	job = client.load_table_from_dataframe(df, table_id, job_config=job_config) 
	
	return None
			</code></pre>
			</br>

			<!-- Tabelas no BigQuery -->
			<h2>Tables in BigQuery</h2>
			<p>
				Finally, the <code>tempo_real</code> and <code>tempo_forecast</code> tables were created in BigQuery and populated with the data obtained from the API.
			</p>
			<p>
				<strong><code>tempo_real</code> table:</strong>
			</p>
			<span class="image fit no-overlay"><img src="images/projetos/projeto-api/05_11.png" alt=""></span>
			<p>
				<strong><code>tempo_forecast</code> table:</strong>
			</p>
			<span class="image fit no-overlay"><img src="images/projetos/projeto-api/05_10.png" alt=""></span>
			<p>
				This setup allows SQL queries to be performed for analyzing the retrieved data.
			</p>
			<span class="image fit no-overlay"><img src="images/projetos/projeto-api/05_12.png" alt=""></span>
		</section>
		<ul class="actions">
			<li><a href="portfolio.html" class="button icon solid fa-arrow-left">Back</a></li>
		</ul>
	</div>

	<!-- Footer -->
	<footer id="footer">
		<div class="inner">
			<ul class="icons">
				<li><a href="https://www.linkedin.com/in/carolyumi" class="icon brands fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
				<li><a href="https://github.com/carolysg" class="icon brands fa-github"><span class="label">Github</span></a></li>
				<li><a href="mailto:carolinagoshima@gmail.com" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
			</ul>
			<ul class="copyright">
				<li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
			</ul>
		</div>
	</footer>

	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>
	<script src="assets/js/prism.js"></script>

</body>

</html>