<!DOCTYPE HTML>
<!--
	Strata by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title>Projeto cervejarias</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="icon" type="image/x-icon" href="/images/icons/favicon.ico">
	<link rel="stylesheet" href="assets/css/main.css" />

	<link rel="stylesheet" href="assets/css/prism.css" />
</head>

<body class="is-preload">
	<script>
		document.body.classList.add('fade');

		document.addEventListener("DOMContentLoaded", () => {
			window.setTimeout(function () {
				document.body.classList.remove('fade');
			}, 230);
		});
	</script>
	<!-- Header -->
	<header id="header">
		<div class="bg-header"></div>
		<div class="inner">
			<a href="#" class="image avatar"><img src="images/img01.png" alt="" /></a>
			<ul class="sidebar">
				<li><a href="index.html" class="sidebar-link">Início</a></li>
				<li><a href="sobre.html" class="sidebar-link">Sobre mim</a></li>
				<li><a href="portfolio.html" class="sidebar-link">Projetos</a></li>
				<li><a href="certificados.html" class="sidebar-link">Certificados</a></li>
			</ul>
		</div>
	</header>

	<!-- Main -->
	<div id="main">
		<section id="projeto-header">
			<header class="major">
				<div class="col-12"></div>
				<h2>Extração e análise de dados de cervejarias</h2>
			</header>
			<ul class="actions fit small">
				<li>
					<a href="https://github.com/carolysg/breweries"
						class="button icon brands small fit fa-github solid" target="_blank">Repositório do Github</a>
				</li>
				<li>
					<a href="https://www.openbrewerydb.org/" class="button fit small" target="_blank">Site da API</a>
				</li>
			</ul>
		</section>

		<!-- Conteúdo vai aqui -->
		<section id="sobre">
			<!-- Sobre o projeto -->
			<h2>Sobre o projeto</h2>
			<p> <span class="image right"><img src="images/thumbs_proj/breweries.jpg" alt=""></span>
				Este projeto consiste em um pipeline projetado para extrair, processar e armazenar dados de cervejarias obtidos pela API Open Brewery DB. O projeto utiliza Apache Airflow para orquestrar o pipeline de dados, garantindo que cada etapa seja executada de maneira confiável e repetível. Neste projeto, as seguintes ferramentas foram utilizadas:
			</p>
			<ul>
				<li>VSCode: editor de código usado para desenvolvimento.</li>
				<li>Git/GitHub: controle de versão.</li>
				<li>Docker: containerização para ambientes consistentes.</li>
				<li>Postgres: banco de dados relacional para persistir os dados agregados.</li>
				<li>Airflow: ferramenta de gerenciamento de fluxo de trabalho para agendamento e monitoramento do pipeline.</li>
				<li>Python 3.9: linguagem de programação utilizada para o desenvolvimento.</li>
			</ul>
			<p> A arquitetura do projeto é apresentada no fluxograma abaixo: </p>
			<span class="image fit no-overlay"><img src="images/projetos/cervejarias/06_01.png" alt=""></span>

			<!-- Estrutura -->
			<h2>Estrutura do projeto</h2>
			<p>O processo de ETL começa extraindo dados da API, armazenando-os em três estágios (bronze, prata e ouro) e, finalmente, carregando-os em um banco de dados PostgreSQL. O Apache Airflow orquestra todo o pipeline, gerenciando o fluxo desde a extração até a transformação e carregamento.</p>
			<ul>
				<li>API: os dados são obtidos de uma fonte externa usando requests em um script Python.</li>
				<li>Bronze: os dados brutos são salvos no formato JSON.</li>
				<li>Prata: os dados transformados são armazenados.</li>
				<li>Ouro: os dados agregados são armazenados aqui. O resultado final é uma tabela agregada com a quantidade de cervejarias por tipo e localização.</li>
				<li>Postgres: os dados agregados são carregados no banco de dados PostgreSQL para uso posterior.</li>
			</ul>
			<p>Para garantir a confiabilidade do pipeline, foram implementados mecanismos de monitoramento e alerta. Os registros são integrados em todo o processo para rastrear sucessos e falhas, enquanto o tratamento de erros captura quaisquer problemas durante a extração, transformação ou carregamento de dados.</p>
			<p>O Airflow está programado para ser executado todos os dias, com um <code>schedule_interval</code> definido como <code>timedelta(days=1)</code>. Este intervalo pode ser alterado na definição da DAG.</p>
			<p>A estrutura do projeto está organizada da seguinte forma:</p>
			<pre><code class="language-python">
				breweries/
				├── dags/
				│   └── brewery_pipeline.py     # DAG principal
				├── data/
				│   ├── bronze/                 # Dados brutos
				│   ├── silver/                 # Dados processados
				│   └── gold/                   # Dados agregados
				├── docs/                      # Documentações extras sobre os dados
				├── images/                    # Imagens
				├── logs/                      # Logs de execução
				├── scripts/       
				│   └── airflow-entrypoint.sh   # Script para o Airflow
				├── requirements.txt           # Dependências
				├── .env                       # Variáveis de ambiente
				├── Dockerfile                 # Dockerfile para Airflow e dependências
				├── docker-compose.yml         # Configurações do Docker Compose
				└── README.md                  # Documentação
			</code></pre>
			<br>

			<!-- Configurações -->
			<h2>Arquivos de configuração</h2>
			<p>Neste projeto, os seguintes arquivos de configuração foram utilizados:</p>
			<ul>
				<li><code>.env</code>: variáveis de conexão com o banco de dados Postgres.</li>
				<li><code>Dockerfile</code>: definições do contêiner para execução do Airflow.</li>
				<li><code>docker-compose.yml</code>: configurações do Docker Compose, responsável pelos ambientes do Postgres e server.</li>
				<li><code>/scripts/airflow-entrypoint.sh</code>: script de execução do Airflow.</li>
				<li><code>requirements.txt</code>: todas as bibliotecas necessárias para a execução do pipeline.</li>
			</ul>

			<!-- Extração e ingestão dos dados -->
			<h2>Extração, processamento e agregação dos dados</h2>
			<p>Todas as operações foram orquestradas pela DAG principal no Airflow. O primeiro passo na construção do script foi realizar as importações das bibliotecas, buscar as variáveis de ambiente disponíveis no arquivo <code>.env</code> e definir alguns argumentos de execução:</p>
			<pre><code class="language-python">
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import requests
import pandas as pd
import numpy as np
import psycopg2
from psycopg2.extras import execute_values
from dotenv import load_dotenv
import os
import logging

logging.basicConfig(level=logging.INFO)

load_dotenv()

db_config = {
    'database': os.getenv("AIRFLOW__CORE__PSYCOPG2_DB"),
    'user': os.getenv("AIRFLOW__CORE__PSYCOPG2_USER"),
    'password': os.getenv("AIRFLOW__CORE__PSYCOPG2_PASS"),
    'host': os.getenv("AIRFLOW__CORE__PSYCOPG2_HOST"),
    'port': os.getenv("AIRFLOW__CORE__PSYCOPG2_PORT"),
}

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 10, 12),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}
			</code></pre><br>
			<p>A seguir, foram definidas duas funções: uma para a criação dos diretórios bronze, silver e gold, e outra para a extração dos dados pela API. O arquivo final obtido foi extraído no formato JSON e salvo na pasta bronze:</p>
			<pre><code class="language-python">
def create_dirs():
	os.makedirs('./data/bronze', exist_ok=True)
	os.makedirs('./data/silver', exist_ok=True)
	os.makedirs('./data/gold', exist_ok=True)

def extract_brewery_data():
	create_dirs()
	try:
		response_total = requests.get("https://api.openbrewerydb.org/breweries/meta")
		total = int(response_total.json()['total'])
		per_page = 200
		num_page = int(np.ceil(total / per_page))
		df = pd.DataFrame()
		for page in range(num_page):
			params = {
				"page": page + 1,
				"per_page": per_page
			}
			response = requests.get("https://api.openbrewerydb.org/breweries", params=params)
			response.raise_for_status()
			data = response.json()
			df_sample = pd.DataFrame(data)
			df = pd.concat([df, df_sample], ignore_index=True)
		df.to_json('./data/bronze/breweries_raw.json', orient='records')
		logging.info("Brewery data extracted successfully.")
	except requests.exceptions.RequestException as e:
		logging.error(f"Error extracting data: {e}")
		raise
			</code></pre><br>
			<p>Posteriormente, a função de transformação dos dados faz a leitura do arquivo na pasta bronze e realiza alguns processamentos nos dados: exclusão de colunas vazias (caso existam), transformação de colunas em tipo float, transformação de caracteres em letras minúsculas e exclusão de colunas repetidas. Os dados transformados são salvos na pasta silver em formato parquet, particionados por país e estado/província.</p>
			<pre><code class="language-python">
def transform_data():
	try:
		df = pd.read_json('./data/bronze/breweries_raw.json')
		df.dropna(axis=1, how='all', inplace=True)
		df['longitude'] = df['longitude'].astype(float)
		df['latitude'] = df['latitude'].astype(float)
		df_final = df.apply(lambda x: x.str.lower() if x.dtype == "object" else x)
		if df_final['state_province'].equals(df_final['state']):
			df_final.drop('state', axis=1, inplace=True)
		df_final.to_parquet('./data/silver/breweries_transformed.parquet', 
								partition_cols=['country', 'state_province'], 
								existing_data_behavior='delete_matching')
		logging.info("Data transformed successfully.")
	except Exception as e:
		logging.error(f"Error transforming data: {e}")
		raise
			</code></pre><br>
			<p>A partir dos dados transformados, uma função de agregação dos dados é responsável por agrupar os dados por estado e por tipo, resultando em uma tabela com três colunas: estado/província, tipo de cervejaria e quantidade de cervejarias. O arquivo final é salvo em formato parquet na pasta gold.</p>
			<pre><code class="language-python">
def aggregate_data():
	try:
		df = pd.read_parquet('./data/silver/breweries_transformed.parquet')
		df_grouped = df.groupby(['state_province', 'brewery_type']).size().reset_index(name='brewery_count')
		df_grouped = df_grouped[df_grouped['brewery_count'] > 0]
		df_grouped.to_parquet('./data/gold/brewery_aggregated_state_province.parquet')
		logging.info("Data aggregated successfully.")
	except Exception as e:
		logging.error(f"Error aggregating data: {e}")
		raise
			</code></pre><br>

			<!-- Dados persistidos no Postgres -->
			<h2>Postgres</h2>
			<p>A tabela final presente na camada gold também é carregada em um banco de dados Postgres, com o intuito de facilitar consultas analíticas e integrar os dados a outras aplicações (caso seja necessário).</p>
			<pre><code class="language-python">
def create_breweries_table(conn):
	with conn.cursor() as cursor:
		cursor.execute("""
			CREATE TABLE IF NOT EXISTS breweries (
				state_province VARCHAR(100),
				brewery_type VARCHAR(100),
				brewery_count INTEGER
			);
		""")

def load_to_postgres():
	try:
		df = pd.read_parquet('./data/gold/brewery_aggregated_state_province.parquet')
		with psycopg2.connect(**db_config) as conn:
			create_breweries_table(conn)
			with conn.cursor() as cursor:
				cursor.execute("TRUNCATE TABLE breweries;")
				execute_values(
					cur=cursor,
					sql="""
						INSERT INTO breweries
						(state_province, brewery_type, brewery_count)
						VALUES %s;
					""",
					argslist=df.to_dict(orient="records"),
					template="""
						(%(state_province)s, %(brewery_type)s, %(brewery_count)s)
					"""
				)
		logging.info("Data loaded into PostgreSQL successfully.")
	except Exception as e:
		logging.error(f"Error loading data into PostgreSQL: {e}")
		raise
			</code></pre><br>

			<!-- DAG -->
			<h2>Execução das tarefas</h2>
			<p>Por fim, cada uma das tasks foi criada utilizando o PythonOperator. As tasks são executadas de forma sequencial:</p>
			<pre><code class="language-python">
with DAG('brewery_pipeline', default_args=default_args, schedule_interval=timedelta(days=1)) as dag:

	extract_task = PythonOperator(
		task_id='extract_data',
		python_callable=extract_brewery_data
	)

	transform_task = PythonOperator(
		task_id='transform_data',
		python_callable=transform_data
	)

	aggregate_task = PythonOperator(
		task_id='aggregate_data',
		python_callable=aggregate_data
	)

	load_task = PythonOperator(
		task_id='load_to_postgres',
		python_callable=load_to_postgres
	)

	extract_task >> transform_task >> aggregate_task >> load_task
			</code></pre><br>

			<!-- Instruções -->
			<h2>Instruções de execução local</h2>
			<p>Caso tenha interesse, o repositório no Github apresenta um passo a passo para a execução do projeto localmente.</p>

		</section>

		<ul class="actions">
			<li><a href="portfolio.html" class="button icon solid fa-arrow-left">Voltar</a></li>
		</ul>
	</div>

	<!-- Footer -->
	<footer id="footer">
		<div class="inner">
			<ul class="icons">
				<li><a href="https://www.linkedin.com/in/carolyumi" class="icon brands fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
				<li><a href="https://github.com/carolysg" class="icon brands fa-github"><span class="label">Github</span></a></li>
				<li><a href="mailto:carolinagoshima@gmail.com" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
			</ul>
			<ul class="copyright">
				<li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
			</ul>
		</div>
	</footer>

	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>
	<script src="assets/js/prism.js"></script>

</body>

</html>