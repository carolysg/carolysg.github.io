<!DOCTYPE HTML>
<!--
	Strata by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title>Brewery project</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="icon" type="image/x-icon" href="/images/icons/favicon.ico">
	<link rel="stylesheet" href="assets/css/main.css" />

	<link rel="stylesheet" href="assets/css/prism.css" />
</head>

<body class="is-preload">
	<script>
		document.body.classList.add('fade');

		document.addEventListener("DOMContentLoaded", () => {
			window.setTimeout(function () {
				document.body.classList.remove('fade');
			}, 230);
		});
	</script>
	<!-- Header -->
	<header id="header">
		<div class="bg-header"></div>
		<div class="inner">
			<a href="#" class="image avatar"><img src="images/img01.png" alt="" /></a>
			<ul class="sidebar">
				<li><a href="index.html" class="sidebar-link">Home</a></li>
				<li><a href="sobre.html" class="sidebar-link">About me</a></li>
				<li><a href="portfolio.html" class="sidebar-link">Projects</a></li>
				<li><a href="certificados.html" class="sidebar-link">Certificates</a></li>
			</ul>
		</div>
	</header>

	<!-- Main -->
	<div id="main">
		<section id="projeto-header">
			<header class="major">
				<div class="col-12"></div>
				<h2>Brewery data extraction and analysis</h2>
			</header>
			<ul class="actions fit small">
				<li>
					<a href="https://github.com/carolysg/breweries"
						class="button icon brands small fit fa-github solid" target="_blank">Github repository</a>
				</li>
				<li>
					<a href="https://www.openbrewerydb.org/" class="button fit small" target="_blank">API website</a>
				</li>
			</ul>
		</section>

		<!-- Conteúdo vai aqui -->
		<section id="sobre">
			<!-- Sobre o projeto -->
			<h2>About the project</h2>
			<p> <span class="image right"><img src="images/thumbs/cervejaria.png" alt=""></span>
				This project involves a pipeline designed to extract, process, and store data on breweries obtained from the Open Brewery DB API. The project uses Apache Airflow to orchestrate the data pipeline, ensuring that each step is executed reliably and consistently. The following tools were used in this project:
			</p>
			<ul>
				<li>VSCode: the code editor used for development.</li>
				<li>Git/GitHub: for version control.</li>
				<li>Docker: for containerization to ensure consistent environments.</li>
				<li>Postgres: a relational database to store the aggregated data.</li>
				<li>Airflow: a workflow management tool for scheduling and monitoring the pipeline.</li>
				<li>Python 3.9: the programming language used for development.</li>
			</ul>
			<p> The project architecture is presented in the diagram below: </p>
			<span class="image fit no-overlay"><img src="images/projetos/cervejarias/06_01.png" alt=""></span>

			<!-- Estrutura -->
			<h2>Project structure</h2>
			<p>
				The ETL process begins by extracting data from the API, storing it in three stages (bronze, silver, and gold), and finally loading it into a PostgreSQL database. Apache Airflow orchestrates the entire pipeline, managing the flow from extraction to transformation and loading.
			</p>
			<ul>
				<li>API: data is obtained from an external source using Python requests in a script.</li>
				<li>Bronze: raw data is saved in JSON format.</li>
				<li>Silver: transformed data is stored here.</li>
				<li>Gold: aggregated data is stored here. The final result is an aggregated table showing the number of breweries by type and location.</li>
				<li>Postgres: aggregated data is loaded into the PostgreSQL database for further use.</li>
			</ul>
			<p>
				To ensure the reliability of the pipeline, monitoring and alerting mechanisms have been implemented. Logs are integrated throughout the process to track successes and failures, while error handling captures any issues that arise during extraction, transformation, or loading of data.
			</p>
			<p>
				Airflow is scheduled to run every day, with a <code>schedule_interval</code> set to <code>timedelta(days=1)</code>. This interval can be changed in the DAG definition.
			</p>
			<p>
				The project structure is organized as follows:
			</p>
			<pre><code class="language-python">
				breweries/
				├── dags/
				│   └── brewery_pipeline.py     # Main DAG
				├── data/
				│   ├── bronze/                 # Raw data
				│   ├── silver/                 # Processed data
				│   └── gold/                   # Aggregated data
				├── docs/                      # Additional data documentation
				├── images/                    # Images
				├── logs/                      # Execution logs
				├── scripts/       
				│   └── airflow-entrypoint.sh   # Script for Airflow
				├── requirements.txt           # Dependencies
				├── .env                       # Environment variables
				├── Dockerfile                 # Dockerfile for Airflow and dependencies
				├── docker-compose.yml         # Docker Compose configurations
				└── README.md                  # Documentation
			</code></pre>
			<br>

			<!-- Configurações -->
			<h2>Configuration files</h2>
			<p>
				In this project, the following configuration files were used:
			</p>
			<ul>
				<li><code>.env</code>: contains the connection variables for the Postgres database.</li>
				<li><code>Dockerfile</code>: defines the container settings for running Airflow.</li>
				<li><code>docker-compose.yml</code>: Docker Compose configurations, managing the Postgres and server environments.</li>
				<li><code>/scripts/airflow-entrypoint.sh</code>: script for executing Airflow.</li>
				<li><code>requirements.txt</code>: lists all the libraries required for running the pipeline.</li>
			</ul>

			<!-- Extração e ingestão dos dados -->
			<h2>Data extraction, processing and aggregation</h2>
			<p>
				All operations were orchestrated by the main DAG in Airflow. The first step in building the script was to import the necessary libraries, load the environment variables from the <code>.env</code> file, and define some execution arguments:
			</p>
			<pre><code class="language-python">
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import requests
import pandas as pd
import numpy as np
import psycopg2
from psycopg2.extras import execute_values
from dotenv import load_dotenv
import os
import logging

logging.basicConfig(level=logging.INFO)

load_dotenv()

db_config = {
    'database': os.getenv("AIRFLOW__CORE__PSYCOPG2_DB"),
    'user': os.getenv("AIRFLOW__CORE__PSYCOPG2_USER"),
    'password': os.getenv("AIRFLOW__CORE__PSYCOPG2_PASS"),
    'host': os.getenv("AIRFLOW__CORE__PSYCOPG2_HOST"),
    'port': os.getenv("AIRFLOW__CORE__PSYCOPG2_PORT"),
}

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 10, 12),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}
			</code></pre><br>
			<p>
				Next, two functions were defined: one for creating the bronze, silver, and gold directories, and another for extracting data from the API. The final file obtained was extracted in JSON format and saved in the bronze folder:
			</p>
			<pre><code class="language-python">
def create_dirs():
	os.makedirs('./data/bronze', exist_ok=True)
	os.makedirs('./data/silver', exist_ok=True)
	os.makedirs('./data/gold', exist_ok=True)

def extract_brewery_data():
	create_dirs()
	try:
		response_total = requests.get("https://api.openbrewerydb.org/breweries/meta")
		total = int(response_total.json()['total'])
		per_page = 200
		num_page = int(np.ceil(total / per_page))
		df = pd.DataFrame()
		for page in range(num_page):
			params = {
				"page": page + 1,
				"per_page": per_page
			}
			response = requests.get("https://api.openbrewerydb.org/breweries", params=params)
			response.raise_for_status()
			data = response.json()
			df_sample = pd.DataFrame(data)
			df = pd.concat([df, df_sample], ignore_index=True)
		df.to_json('./data/bronze/breweries_raw.json', orient='records')
		logging.info("Brewery data extracted successfully.")
	except requests.exceptions.RequestException as e:
		logging.error(f"Error extracting data: {e}")
		raise
			</code></pre><br>
			<p>
				Afterwards, the data transformation function reads the file from the bronze folder and performs several processing steps: removing empty columns (if any), converting columns to float type, transforming characters to lowercase, and removing duplicate columns. The transformed data is saved in the silver folder in Parquet format, partitioned by country and state/province.
			</p>
			<pre><code class="language-python">
def transform_data():
	try:
		df = pd.read_json('./data/bronze/breweries_raw.json')
		df.dropna(axis=1, how='all', inplace=True)
		df['longitude'] = df['longitude'].astype(float)
		df['latitude'] = df['latitude'].astype(float)
		df_final = df.apply(lambda x: x.str.lower() if x.dtype == "object" else x)
		if df_final['state_province'].equals(df_final['state']):
			df_final.drop('state', axis=1, inplace=True)
		df_final.to_parquet('./data/silver/breweries_transformed.parquet', 
								partition_cols=['country', 'state_province'], 
								existing_data_behavior='delete_matching')
		logging.info("Data transformed successfully.")
	except Exception as e:
		logging.error(f"Error transforming data: {e}")
		raise
			</code></pre><br>
			<p>
				Based on the transformed data, an aggregation function groups the data by state and type, resulting in a table with three columns: state/province, brewery type, and the number of breweries. The final file is saved in Parquet format in the gold folder.
			</p>
			<pre><code class="language-python">
def aggregate_data():
	try:
		df = pd.read_parquet('./data/silver/breweries_transformed.parquet')
		df_grouped = df.groupby(['state_province', 'brewery_type']).size().reset_index(name='brewery_count')
		df_grouped = df_grouped[df_grouped['brewery_count'] > 0]
		df_grouped.to_parquet('./data/gold/brewery_aggregated_state_province.parquet')
		logging.info("Data aggregated successfully.")
	except Exception as e:
		logging.error(f"Error aggregating data: {e}")
		raise
			</code></pre><br>

			<!-- Dados persistidos no Postgres -->
			<h2>Postgres</h2>
			<p>
				The final table in the gold layer is also loaded into a Postgres database to facilitate analytical queries and integrate the data with other applications if needed.
			</p>
			<pre><code class="language-python">
def create_breweries_table(conn):
	with conn.cursor() as cursor:
		cursor.execute("""
			CREATE TABLE IF NOT EXISTS breweries (
				state_province VARCHAR(100),
				brewery_type VARCHAR(100),
				brewery_count INTEGER
			);
		""")

def load_to_postgres():
	try:
		df = pd.read_parquet('./data/gold/brewery_aggregated_state_province.parquet')
		with psycopg2.connect(**db_config) as conn:
			create_breweries_table(conn)
			with conn.cursor() as cursor:
				cursor.execute("TRUNCATE TABLE breweries;")
				execute_values(
					cur=cursor,
					sql="""
						INSERT INTO breweries
						(state_province, brewery_type, brewery_count)
						VALUES %s;
					""",
					argslist=df.to_dict(orient="records"),
					template="""
						(%(state_province)s, %(brewery_type)s, %(brewery_count)s)
					"""
				)
		logging.info("Data loaded into PostgreSQL successfully.")
	except Exception as e:
		logging.error(f"Error loading data into PostgreSQL: {e}")
		raise
			</code></pre><br>

			<!-- DAG -->
			<h2>Task execution</h2>
			<p>
				Finally, each of the tasks was created using the PythonOperator. The tasks are executed sequentially:
			</p>
			<pre><code class="language-python">
with DAG('brewery_pipeline', default_args=default_args, schedule_interval=timedelta(days=1)) as dag:

	extract_task = PythonOperator(
		task_id='extract_data',
		python_callable=extract_brewery_data
	)

	transform_task = PythonOperator(
		task_id='transform_data',
		python_callable=transform_data
	)

	aggregate_task = PythonOperator(
		task_id='aggregate_data',
		python_callable=aggregate_data
	)

	load_task = PythonOperator(
		task_id='load_to_postgres',
		python_callable=load_to_postgres
	)

	extract_task >> transform_task >> aggregate_task >> load_task
			</code></pre><br>

			<!-- Instruções -->
			<h2>Local execution instructions</h2>
			<p>
				If you're interested, the repository on GitHub provides a step-by-step guide for running the project locally.
			</p>
		</section>

		<ul class="actions">
			<li><a href="portfolio.html" class="button icon solid fa-arrow-left">Back</a></li>
		</ul>
	</div>

	<!-- Footer -->
	<footer id="footer">
		<div class="inner">
			<ul class="icons">
				<li><a href="https://www.linkedin.com/in/carolyumi" class="icon brands fa-linkedin-in"><span class="label">LinkedIn</span></a></li>
				<li><a href="https://github.com/carolysg" class="icon brands fa-github"><span class="label">Github</span></a></li>
				<li><a href="mailto:carolinagoshima@gmail.com" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
			</ul>
			<ul class="copyright">
				<li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
			</ul>
		</div>
	</footer>

	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>
	<script src="assets/js/prism.js"></script>

</body>

</html>